// Copyright 2025 Peter Beverloo & AnimeCon. All rights reserved.
// Use of this source code is governed by a MIT license that can be found in the LICENSE file.

import { type Candidate, type GenerateContentParameters, type Part, GoogleGenAI }
    from '@google/genai';

import type { AiSupportedModel } from './Models';
import type { GoogleClient } from '../google/GoogleClient';

/**
 * Settings accepted by the GenAI client.
 */
export interface ClientSettings {
    /**
     * Google Cloud API key that should be used for generative AI.
     */
    apiKey: string;

    /**
     * Maximum number of candidates to respond with when generating text.
     */
    candidateCount: number;

    /**
     * Settings for the Google Client that should be used for authentication.
     */
    googleClient: GoogleClient;

    /**
     * Model selection controlling which models should be used for different use cases.
     */
    models: {
        /**
         * Model to use when generating images.
         */
        image: AiSupportedModel;

        /**
         * Model to use when generating text.
         */
        text: AiSupportedModel;
    };

    /**
     * Quality settings that apply to text prompts, including variability of the output.
     */
    quality: {
        /**
         * Value that controls the degree of randomness in token selection. Lower temperatures are good
         * for prompts that require a less open-ended or creative response, while higher temperatures
         * can lead to more diverse or creative results.
         */
        temperature?: number;

        /**
         * For each token selection step, the `top_k` tokens with the highest probabilities are sampled.
         * Then tokens are further filtered based on `top_p` with the final token selected using
         * temperature sampling.
         */
        topK?: number;

        /**
         * Tokens are selected from the most to least probable until the sum of their probabilities
         * equals this value.
         */
        topP?: number;
    };

    /**
     * Whether to use VertexAI (when `true`), or the Google AI Studio backend (when `false`).
     */
    vertexAi: boolean;
};

/**
 * Subset of the total model response information that captures the result of a request.
 */
type ModelResponse<T extends object> =
    { success: false, error: string } | ({
        /**
         * Boolean indicating that a response could be generated.
         */
        success: true;

        /**
         * Unique ID of the response generated by the model, owned by the provider.
         */
        id?: string;

        /**
         * Candidates responded with by the output. There may be more than one.
         */
        candidates: Candidate[];

    } & T);

/**
 * Information returned by the model upon execution of a image prompt.
 */
export type ModelImageResponse = ModelResponse<{
    /**
     * Byte data returned by the model, as a base64-encoded PNG. Guaranteed to exist.
     */
    data: string;
}>;

/**
 * Information returned by the model upon execution of a text prompt.
 */
export type ModelTextResponse = ModelResponse<{
    /**
     * Textual output of the model. Guaranteed to exist.
     */
    text: string;
}>;

/**
 * Parameters that can be passed when requesting a model.
 */
interface GenerateRequest {
    /**
     * One or more attachments that should be included with the prompt. Use raw byte data, and make
     * sure that the right mime type is set on the entry.
     */
    attachments?: {
        /**
         * Source data, as raw, unencoded bytes. Will be encoded as required by the client.
         */
        bytes: Uint8Array<ArrayBuffer> | Promise<Uint8Array<ArrayBuffer>>;

        /**
         * The IANA standard MIME type of the source data.
         */
        mimeType: string;

    }[];

    /**
     * The prompt that explains to the model what it's supposed to do. (Required.)
     */
    prompt: string;

    /**
     * System prompt, explaining to the model what it is.
     */
    systemPrompt?: string;
}

/**
 * Parameters that can be passed when requesting a model to generate an image.
 */
interface GenerateImageRequest extends GenerateRequest {
    /**
     * Aspect ratio of the image that should be generated.
     */
    aspectRatio: '1:1' | '2:3' | '3:2' | '3:4' | '4:3' | '9:16' | '16:9' | '21:9';
}

/**
 * Client to the Google Gen AI SDK for TypeScript and JavaScript, through which new Gemini features
 * will consistently be exposed. Can switch between the VertexAI and Google AI Studio backends.
 * 
 * @see https://googleapis.github.io/js-genai/release_docs/index.html
 * @see https://www.npmjs.com/package/@google/genai
 */
export class Client {
    #ai: GoogleGenAI;
    #settings: ClientSettings;

    constructor(settings: ClientSettings) {
        this.#ai = new GoogleGenAI({
            vertexai: settings.vertexAi,
            ...(settings.vertexAi ? {
                // VertexAI:
                googleAuthOptions: {
                    credentials: JSON.parse(settings.googleClient.credentials),
                    scopes: 'https://www.googleapis.com/auth/cloud-platform',
                },
                location: settings.googleClient.location,
                project: settings.googleClient.projectId,

            } : {
                // Google AI Studio:
                apiKey: settings.apiKey,
            })
        });

        this.#settings = settings;
    }

    /**
     * Uses the model to generate an image, in accordance with the given |request|. This causes an
     * API call over the internet, and may take an arbitrary amount of time to complete.
     */
    async generateImage(request: GenerateImageRequest): Promise<ModelImageResponse> {
        const generator = await this.generateContent(request, {
            model: this.#settings.models.image,
            config: {
                candidateCount: /* fixed= */ 1,
                systemInstruction: request.systemPrompt,
                imageConfig: {
                    aspectRatio: request.aspectRatio,
                },
            },
        });

        if (!generator.success || !generator.response)
            return { success: false, error: generator.error };

        const { response } = generator;
        return {
            success: true,
            id: response.responseId!,
            candidates: response.candidates!,
            data: response.data!,
        };
    }

    /**
     * Uses the model to generate text, in accordance with the given |request|. This causes an API
     * call over the internet, and may take an arbitrary amount of time to complete.
     */
    async generateText(request: GenerateRequest): Promise<ModelTextResponse> {
        const generator = await this.generateContent(request, {
            model: this.#settings.models.text,
            config: {
                candidateCount: this.#settings.candidateCount,
                temperature: this.#settings.quality.temperature,
                topK: this.#settings.quality.topK,
                topP: this.#settings.quality.topP,
                systemInstruction: request.systemPrompt,
            },
        });

        if (!generator.success || !generator.response)
            return { success: false, error: generator.error };

        const { response } = generator;
        return {
            success: true,
            id: response.responseId!,
            candidates: response.candidates!,
            text: response.text!,
        };
    }

    /**
     * Private method that actually interacts with the model to generate the requested content. Will
     * inject the prompt and any attachments into the prompt, and verify that the required
     * information (unique ID and candidates) are present.
     */
    private async generateContent(
        request: GenerateRequest, parameters: Omit<GenerateContentParameters, 'contents'>)
    {
        try {
            const attachments: Part[] = [ /* none yet */ ];
            if (!!request.attachments) {
                for (const attachment of request.attachments) {
                    const bytes = await attachment.bytes;
                    const encodedBytes = Buffer.from(bytes).toString('base64');

                    attachments.push({
                        inlineData: {
                            data: encodedBytes,
                            mimeType: attachment.mimeType,
                        },
                    });
                }
            }

            const modelResponse = await this.#ai.models.generateContent({
                contents: [
                    {
                        text: request.prompt,
                    },
                    ...attachments,
                ],
                ...parameters,
            });
            if (!modelResponse.responseId)
                throw new Error(`The model did not throw, but also did not return a unique ID`);

            if (!modelResponse.candidates?.length)
                throw new Error(`The model did not throw, but also did not return any candidates`);

            return { success: true, response: modelResponse };

        } catch (error: any) {
            return { success: false, error: error.toString() };
        }
    }
}
